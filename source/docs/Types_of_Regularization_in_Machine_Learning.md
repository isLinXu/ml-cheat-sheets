# 机器学习中的正则化类型

机器学习正则化初学者指南。

在本文中，我们将介绍什么是正则化，为什么需要它，以及机器学习模型中常用的正则化类型有哪些。

# 为什么要正则化？

正则化通常用作机器学习中过度拟合问题的解决方案。过度拟合的常见原因是

1. 当模型足够复杂时，它开始对训练数据中的噪声进行建模。
2. 当训练数据相对较小并且不能充分表示从中采样的基础分布时，模型无法学习可概括的映射。

![img](https://miro.medium.com/max/1050/1*vCGSqy05QbiYhvdH4DXxBw.png)

机器学习中的过度拟合——图片来自作者

正则化帮助我们克服过度拟合的问题。

# 什么是正则化？

*正则化由不同的技术和方法组成，用于通过减少泛化误差而不影响训练误差来解决过度拟合问题**.* 为训练数据点选择过于复杂的模型通常会导致过度拟合。另一方面，更简单的模型会导致数据欠拟合。因此，在模型中选择合适的复杂度至关重要。由于模型的复杂性不能直接从可用的训练数据中推断出来，因此通常不可能偶然发现正确的模型复杂性来进行训练。

这就是正则化发挥作用的地方，使复杂模型容易过度拟合。

# 正则化的类型

根据克服过拟合的方法，我们可以将正则化技术分为三类。根据方法在解决过度拟合问题方面的有效性，每种正则化方法被标记为强、中和弱。

## 1.修改损失函数

在这些正则化技术中，优化模型的损失函数被修改为直接考虑学习参数或输出分布的范数。我们有以下基于损失函数的正则化技术。

## A。L2 正则化（强）：

考虑以下具有均方损失的线性回归问题。

![img](https://miro.medium.com/max/1050/1*lI-jrI2hMQiuRcDSokE5mw.png)

在 L2 正则化中，我们修改损失以包括正在优化的权重 (beta) 的加权 L2 范数。这可以防止权重变得太大，从而避免它们过度拟合。

![img](https://miro.medium.com/max/1050/1*zFuomRDJFeqtMy1sbekywQ.png)

使用 L2 正则化修改损失 — 图片来自 Author

常数 lambda (≥0) 用于控制过度拟合和欠拟合之间的折衷。当 lambda 高（低）时，模型容易欠拟合（过拟合）。

让我们考虑二维情况 (n=2)，我们可以在其中可视化笛卡尔平面中的回归。下图在笛卡尔平面上绘制了损失函数的两部分（MSE 损失和 L2 正则化）。L2 正则化器被绘制为一个半径等于 lambda 的圆。MSE 损失已绘制为等值线图，其中损失在椭圆上相等。解决上面显示的 L2 正则化损失的权重在视觉上意味着找到位于绿色球内的 MSE 轮廓（蓝色）上具有最小损失的点。增加 lambda 的值对应于增加绿色球的大小。当 lambda 增加超过某个点时，绿色球开始与实际未调节的最小值点相交（在图中显示为“没有正则化的最佳 beta_0，beta_1”）。这意味着将 lambda 增加到超出此范围不会改变我们的解决方案。对于解决过拟合问题的模型，选择的 lambda 值必须介于此 lambda 值和 0 之间。

![img](https://miro.medium.com/max/1050/1*lFmQebk5GoibKN2ZQYzYpw.png)

解决上面显示的 L2 正则化损失的权重在视觉上意味着找到位于 L2 球（绿色圆圈）内的 MSE 轮廓（蓝色）上损失最小的点。

## b. L1 正则化（强）：

在 L1 正则化中，不使用损失函数中权重的 L2 范数，而是使用权重的 L1 范数（绝对值）。修改后的损失变为

![img](https://miro.medium.com/max/1050/1*oYyGL3TFuKkh5xTlcPrVSQ.png)

L1 正则化修正损失 — 图片作者：Author

就像 L2 正则化器一样，L1 正则化器在 MSE 等高线图上找到位于单位范数球内的损失最小的点。L1 范数的单位范数球是一个有边的菱形。从视觉上看，这可以在下图中看到。

![img](https://miro.medium.com/max/1050/1*P3bTxrYuLGhF5-m0WEgomQ.png)

解决上面显示的 L1 正则化损失的权重在视觉上意味着找到位于 L1 球（贪婪菱形）内的 MSE 轮廓（蓝色）上损失最小的点。

使用 L1 正则化器优于 L2 正则化器的额外优势是 L1 范数往往会导致权重稀疏。这意味着，使用这样的正则化器，权重 beta 可能包含零元素。L2 正则化器的权重可以变得非常小，但它们实际上永远不会变为零。

增加 lambda 的值会增加菱形的大小，从而减少实际的 MSE 损失。这意味着权重开始越来越适合数据点，这可能导致过度拟合。与 L2 正则化器类似，当 lambda 非常高时，回归模型的未调节最小值与 L1 调节最小值相同。

## C。熵正则化（强）：

熵根据其中的不确定性来量化概率分布。分布的不确定性越大，熵就越大。均匀分布的所有事件发生的概率相等，这意味着不确定性最大，因此熵最大。另一方面，脉冲分布意味着如果从这样的分布中抽取随机事件，我们知道它总是相同的。因此，这种脉冲分布具有最小熵。

当模型的输出是概率分布时使用熵正则化，例如分类、策略梯度强化学习等。熵正则化器不是直接使用损失项中的权重范数，而是包括按比例缩放的输出分布的熵通过拉姆达。

考虑以下分类问题。

![img](https://miro.medium.com/max/1050/1*aMfwFD1vtCXIVuCfPpPxYA.png)

损失函数通常是二元交叉熵或铰链损失。在熵正则化的情况下，损失函数修改如下

![img](https://miro.medium.com/max/1050/1*e22QgKGc8ia9LwOmuBl0OQ.png)

熵正则化修正损失 — 图片作者：Author

由于我们希望输出概率具有一定程度的不确定性，这意味着我们要增加熵。由于我们正在减少损失，因此在损失函数中使用熵需要乘以 -1。缩放常数 lambda 控制正则化。lambda 的值越大，输出分布越均匀。

![img](https://miro.medium.com/max/1050/1*DLZrJNJzOxVSila28qhcJA.png)

熵正则化输出分布 — 图片作者：Author

## 2.修改采样方式

这些方法对于克服由于可用数据集大小有限而引起的过度拟合很有用。这些正则化方法试图操纵可用输入以创建实际输入分布的公平表示。以下是属于此类的两种正则化方法

## A。数据增强（弱）：

数据增强涉及通过随机裁剪、扩张、旋转、添加少量噪声等创建的更多输入来增加可用数据集的大小，如下面的示例图所示。这个想法是人为地创建更多数据，希望增强的数据集能够更好地表示底层隐藏分布。由于我们仅受可用数据集的限制，因此此方法通常不能很好地用作正则化器。

![img](https://miro.medium.com/max/1050/0*xfCOt_Wo0Pa9fodp.png)

图片来源：[https ://www.researchgate.net/publication/319413978](https://www.researchgate.net/publication/319413978)

## b. K 折交叉验证（中等）：

此方法用于创建多个经过训练的网络，然后选择泛化误差最小的网络。在从隐藏分布采样的未见数据集上，具有最小泛化误差的模型有望比其他模型表现更好。

在 K 折交叉验证中，将可用的训练数据集划分为 k 个不重叠的子集，并对 K 个模型进行训练。对于每个模型，k 个子集中的一个用于验证，而其余 (k-1) 个子集用于训练（如下图所示）。该模型一旦经过训练，就会在保留验证子集上进行评估，并记录性能。一旦所有 K 个模型都经过训练并记录了保留验证子集上的性能，则选择具有最佳性能指标的模型作为最终模型。

![img](https://miro.medium.com/max/1050/1*ETZwPbOvu7AQF8ZVUcErzQ.png)

5 折交叉验证 — 图片来自 Author

## 3.修改训练算法

正则化也可以通过以各种方式修改训练算法来实现。下面讨论两种最常用的方法。

## a.Dropout（强）

当训练模型是神经网络时使用 Dropout。神经网络由多个隐藏层组成，其中一层的输出用作后续层的输入。后续层通过可学习参数修改输入（通常通过将其乘以矩阵并添加偏差和激活函数）。输入流经神经网络层，直到到达用于预测的最终输出层。

神经网络中的每一层都由各种节点组成。上一层的节点连接到下一层的节点。在 dropout 方法中，连续层的节点之间的连接根据 dropout-ratio（丢弃的总连接的百分比）随机丢弃，并在当前迭代中训练剩余网络。在下一次迭代中，另一组随机连接被丢弃。

![img](https://miro.medium.com/max/1050/1*WCP9F1g9PG4nlO-S4jMw9A.png)

dropout 方法确保神经网络学习到一组更健壮的特征，这些特征与所选节点的随机子集表现得同样好。通过随机丢弃连接，网络能够学习从输入到输出的更好的泛化映射，从而减少过度拟合。dropout ratio 需要仔细选择，它对学习模型有重大影响。dropout ratio 的一个好的值在 0.25 到 0.4 之间。

## b. 注入噪声（弱）

与dropout类似，这种方法通常在学习的模型是神经网络时使用。在这种方法中，我们篡改了通过反向传播学习的权重，以使其更健壮或对微小变化不敏感。在训练过程中，少量随机噪声被添加到更新的权重中，这有助于模型学习更强大的特征集。一组强大的功能可确保模型不会过度拟合训练数据。但是，这种方法作为正则化器效果不是很好。

# 概括

正则化在机器学习中被用作通过减少所考虑的 ML 模型的方差来解决过度拟合的方法。可以通过修改损失函数、采样方法或训练方法本身以多种方式实现正则化。下面的备忘单总结了不同的正则化方法。

![img](https://miro.medium.com/max/1050/1*KBU0hPg94w2TLa1SUgrAEg.png)

ML Cheatsheet 中的正则化 — 图片来源：[www.cheatsheets.aqeel-anwar.com](http://www.cheatsheets.aqeel-anwar.com/)

