# 局部响应归一化和批量归一化之间的区别

## 关于深度神经网络中使用的不同规范化技术的简短教程。

# 为什么规范化？

归一化对于补偿某些激活函数（例如 ReLU、ELU 等）的无界性质的深度神经网络变得很重要。有了这些激活函数，输出层就不会被限制在有界范围内（例如 [-1,1]对于*tanh*），而是他们可以在训练允许的范围内成长。为了限制无界激活增加输出层值，在激活函数之前使用归一化。深度神经网络中使用了两种常见的归一化技术，并且经常被初学者误解。在本教程中，将讨论这两种规范化技术的详细解释，突出它们的主要区别。

# 本地响应归一化

局部响应归一化 (LRN) 最初是在 AlexNet 架构中引入的，其中使用的激活函数是*ReLU*，而不是当时更常见的*tanh*和*sigmoid 。*除了上述原因外，使用 LRN 的原因是为了鼓励*侧抑制。*它是神经生物学中的一个概念，指的是神经元减少其邻居活动的能力 [1]。在 DNN 中，这种横向抑制的目的是进行局部对比度增强，以便将局部最大像素值用作下一层的激励。

LRN 是一个**不可训练的层**，它对局部邻域内特征图中的像素值进行平方归一化。根据定义的邻域，LRN 有两种类型，如下图所示。

![img](https://miro.medium.com/max/1050/1*MFl0tPjwvc49HirAJZPhEA.png)

**Inter-Channel LRN：**这个原来是AlexNet论文用的。定义的邻域在**通道对面**。对于每个(x,y)位置，在深度维度上进行归一化，由下式给出

![img](https://miro.medium.com/max/1050/1*JXGTZuvplcGpyE8DuP4B2w.png)

AlexNet中使用的LRN [2]

其中*i*表示滤波器i的输出，*a(x,y),b(x,y)分别为归一化前后**(x,y)*位置的像素值，N为通道总数。常数*(k,α,β,n)*是超参数。*k*用于避免任何奇点（被零除），*α*用作归一化常数，而*β*是对比常数。常量*n*用于定义邻域长度，即在执行归一化时需要考虑多少个连续像素值。*( k,α,β,n)=(0,1,1,N)*的情况是标准归一化）。在上图中，当 N=4 时，n 取为 2。

让我们看一个 Inter-channel LRN 的例子。考虑下图

![img](https://miro.medium.com/max/1050/1*DmnOhSTIzn04sC0w1d3FPg.png)

不同的颜色表示不同的通道，因此 N=4。让超参数为 ( *k,α,β,n)=(0,1,1,2)。**n=2*的值意味着在计算位置*(i,x,y)*处的归一化值时，我们考虑前一个和下一个过滤器在相同位置的值，即*(i-1, x, y)*和*(i +1, x, y)*。对于*(i,x,y)=(0,0,0)*我们有*value(i,x,y)=1*，*value(i-1,x,y)*不存在并且*value(i+,x, y)=1*。因此*normalized_value(i,x,y) = 1/(¹²+¹²) = 0.5*并且可以在上图的下半部分看到。其余的归一化值以类似的方式计算。

**Intra-Channel LRN：**在Intra-channel LRN中，邻域仅在同一通道内扩展，如上图所示。公式由下式给出

![img](https://miro.medium.com/max/1050/1*-19IMI2wJVDtaz4Uf4dRog.png)

其中 (W,H) 是特征图的宽度和高度（例如上图中的 (W,H) = (8,8)）。Inter 和 Intra Channel LRN 之间的唯一区别是归一化的邻域。在通道内 LRN 中，在考虑中的像素周围定义了一个 2D 邻域（与 Inter-Channel 中的 1D 邻域相反）。例如，下图显示了 n=2 的 5x5 特征图上的通道内归一化（即以 (x,y) 为中心的大小为 (n+1)x(n+1) 的 2D 邻域）。

![img](https://miro.medium.com/max/1050/1*vFC3KU-wQPG1ZnXFYC_xPQ.png)

# 批量归一化：

批量归一化 (BN) 是一个**可训练层，通常用于解决*****内部协变量偏移 (ICF)\*** [1]的问题*。*ICF 的产生是由于隐藏神经元/激活的变化分布。考虑以下二元分类示例，我们需要对玫瑰和非玫瑰进行分类

![img](https://miro.medium.com/max/1050/1*gUOjaIspVsz-PVxLDLQGQA.png)

玫瑰与非玫瑰分类。右侧绘制的特征图对于从数据集 [1] 中采样的两个不同批次具有不同的分布

假设我们已经训练了一个神经网络，现在我们从数据集中选择两个外观明显不同的批次进行推理（如上所示）。如果我们对这两个批次进行正向传递并绘制隐藏层（网络深处）的特征空间，我们将看到分布发生显着变化，如上图右侧所示。这称为输入神经元的***协变量偏移。\***这对训练有什么影响？在训练期间，如果我们选择属于不同分布的批次，那么它会减慢训练速度，因为对于给定的批次，它会尝试学习特定的分布，而这对于下一批次是不同的。因此它在分布之间不断来回跳动直到收敛。这个***协变量偏移\***可以通过确保批次中的成员不属于相同/相似的分布来缓解。这可以通过为批次随机选择图像来完成。隐藏神经元也存在类似的 Covariate Shift。即使批次是随机选择的，隐藏的神经元最终也会有一定的分布，这会减慢训练速度。这种隐藏层的 Covariate shift 称为 Internal Covariate Shift。问题是我们不能像控制输入神经元那样直接控制隐藏神经元的分布，因为它会随着训练更新训练参数而不断变化。Batch Normalization 有助于缓解这个问题。

在批量归一化中，隐藏神经元的输出在被馈送到激活函数之前按以下方式处理。

1. 将整个批次*B*归一化为零均值和单位方差

- 计算整个小批量输出的均值：*u_B*
- 计算整个mini-batch输出的方差：s *igma_B*
- 通过减去均值并除以方差来归一化小批量

\2. 引入两个可训练参数（*Gamma:* scale_variable 和*Beta:* shift_variable）来缩放和移动归一化的小批量输出

\3. 将这个经过缩放和移位的归一化小批量输入到激活函数中。

BN算法如下图所示。

![img](https://miro.medium.com/max/608/1*Hiq-rLFGDpESpr8QNsJ1jg.png)

批量归一化算法 [2]

对批处理中所有激活的每个像素执行归一化。考虑下图。假设我们有一个大小为 3 的小批量。隐藏层产生大小为 (C,H,W) = (4,4,4) 的激活。由于批量大小为 3，我们将有 3 个这样的激活。现在对于激活中的每个像素（即每个 4x4x4=64 像素），我们将通过找到所有激活中该像素位置的均值和方差来对其进行归一化，如下图左侧所示。一旦找到均值和方差，我们将从每个激活中减去均值并将其除以方差。下图的右侧部分描述了这一点。减法和除法是逐点进行的。（如果您习惯使用 MATLAB，则除法是点除法**./**）。

![img](https://miro.medium.com/max/1050/1*PgUwNzUYs2_Sp5nrPfSZ5g.jpeg)

第 2 步（即缩放和移动）的原因是让训练决定我们是否需要归一化。在某些情况下，不进行标准化可能会产生更好的结果。因此，BN 不是事先选择是否包含归一化层，而是让训练来决定它。当*Gamma = sigma_B*和*Beta = u_B*时，不进行归一化，并恢复原始激活。[可以在这里](https://www.coursera.org/lecture/deep-neural-network/why-does-batch-norm-work-81oTm)找到由 Andrew Ng 编写的关于 BN 的非常好的视频教程

# 比较：

LRN 有多个方向来跨（通道间或通道内）执行归一化，另一方面，BN 只有一种执行方式（对于所有激活的每个像素位置）。下表比较了两种归一化技术。

![img](https://miro.medium.com/max/1050/1*J7rxGz1f_2YWjdcsvqNCNA.png)

**参考：**

[1] https://www.learnopencv.com/batch-normalization-in-deep-networks/

[2] Ioffe、Sergey 和 Christian Szegedy。“批量归一化：通过减少内部协变量偏移来加速深度网络训练。” *arXiv 预印本 arXiv:1502.03167* (2015)。