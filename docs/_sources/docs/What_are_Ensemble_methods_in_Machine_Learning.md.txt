# 机器学习中的集成方法是什么？

使用备忘单直观地浏览机器学习中的集成方法

# 背景

假设您搬到了一个新地方并想外出就餐。你如何找到一个好地方？

**解决方案 1：**找一位真正擅长工作的美食评论家，看看他/她是否对您所在地区的餐厅有任何推荐

**解决方案 2：**使用 Google 并随机查看**一位用户**对几家餐厅的评论。

**解决方案 3：**使用 Google 并查看**多个用户**对几家餐厅的评论，然后对**他们的评分进行平均**。

让我们分析上述每个解决方案。

## 解决方案 1：

1. 食品评论家一般都非常**准确**。
2. 很难**找到**美食评论家
3. 也许您找到的美食评论家是严格的素食主义者，而您不是。那样的话，食评家的推荐就会有失**偏颇**。

## 解决方案 2：

另一方面，在互联网上随机获取一个人对餐厅的星级评分是

1. 不太**准确**
2. **更容易**找到

## 解决方案 3：

1. 总的来说，它可能恰好是您需要的**准确度**
2. **在互联网上更容易**找到
3. **偏见**要少得多，因为对餐厅进行评分的用户来自不同的背景。

因此，无需向美食评论家询问，只需查看一群随机（但人数众多）的人的集体意见，就可以获得相当不错的餐厅推荐。这被称为***群体智慧\****，是Quora、Stack-exchange 和 Wikipedia 等*各种信息网站的支柱。

# 什么是集成方法？

机器学习中的集成方法使用多个弱学习器共同预测输出。您无需为数据集训练一个大型/复杂模型，而是训练多个小型/简单模型（弱学习器）并聚合它们的输出（以各种方式）以形成您的预测，如下图所示

![img](https://miro.medium.com/max/1050/1*Naj64WlDU1L9X3uNBCTf6A.png)

推理：集成方法（作者图片）

# 集成方法的类型

一般来说，目前 ML 中常用的集成方法有 3 种不同类型

1. 套袋
2. 升压
3. 堆叠

这些方法具有相同的***群体智慧\***概念，但在重点**关注的细节、使用的****弱学习器类型**以及用于形成最终输出的**聚合类型方面有所不同。**

## 1.装袋

在Bagging（**Bootstrap** **Agg** erating ）中**，****并行**训练多个弱学习器。对于每个弱学习器，输入数据都是从原始数据集中随机抽样并进行训练的。有放回的子集的随机抽样会产生几乎 iid 的样本。在推理过程中，测试输入被馈送到所有弱学习器并收集输出。最终预测是通过对每个弱学习器的输出进行投票来进行的。

完整的步骤如下方框图所示。

![img](https://miro.medium.com/max/1050/1*Msptl4h9u0MKn5-3ml-qOA.png)

集成方法——Bagging（图片来自作者）

在装袋方法中，弱学习者通常属于同一类型。由于带替换的随机抽样会创建 iid 样本，并且聚合 iid 变量不会改变偏差但会降低方差，因此装袋方法不会改变预测中的偏差但会降低其**方差**。

## 2.助推

在 boosting 中，多个弱学习器被**顺序学习**。每个后续模型都通过更加重视被先前的弱学习者错误分类的数据点来训练。通过这种方式，弱学习者可以专注于特定的数据点，并可以共同减少预测的**偏差。**完整的步骤如下方框图所示。

![img](https://miro.medium.com/max/1050/1*ckjJtevu1tZ0RZCZHGoEVw.png)

集成方法——提升（图片来自作者）

第一个弱学习器通过对数据集中的所有数据点赋予相同的权重进行训练。一旦训练了第一个弱学习器，就会评估**每个点**的预测误差。基于每个数据点的误差，更新下一个学习者的数据点的相应权重。如果数据点被训练的弱学习器正确分类，则其权重降低，否则，其权重增加。除了更新权重之外，每个弱学习器还维护一个标量 alpha，用于量化弱学习器对整个训练数据集进行分类的程度。

随后的模型在这些加权点集上进行训练。对一组加权点进行训练的一种方法是在误差中表示权重项。不使用均方误差，而是使用加权均方误差，以确保具有更高分配权重的数据点在正确分类时更加重要。另一种方法可以是加权采样，即在训练时基于权重的样本点。

在推理阶段，测试输入被馈送到所有弱学习器并记录它们的输出。最终预测是通过在将每个弱学习器的输出与相应的弱学习器的权重 alpha 进行缩放之前，在将它们用于投票之前实现的，如上图所示。

## 3.堆叠

在堆叠中，多个弱学习器被并行训练，这类似于装袋中发生的情况。但与bagging不同的是，stacking并不进行简单的投票来聚合每个weak-learner的输出来计算最终的预测。相反，另一个元学习器在弱学习器的输出上进行训练，以学习从弱学习器输出到最终预测的映射。完整的框图可以在下面看到。

![img](https://miro.medium.com/max/1050/1*lHJ9NOiODiQm9MGOGm41zA.png)

集成方法——堆叠（图片来自作者）

Stacking 通常有不同类型的弱学习器。因此，一种简单的投票方法为所有弱学习器预测赋予相同的权重似乎不是一个好主意（如果弱学习器在结构上是相同的）。这就是元学习器的用武之地。它试图了解哪个弱学习器更重要。

弱学习器是并行训练的，而元学习器是顺序训练的。一旦训练了弱学习器，它们的权重就保持不变以训练元学习器。通常，元学习器在与用于训练弱学习器的子集不同的子集上进行训练。

# 备忘单

以下备忘单涵盖了可能会派上用场的 Ensemble 方法主题。

![img](https://miro.medium.com/max/1050/1*TkXkL0mpzWB1PJcHrK3FOg.png)

备忘单集成方法（来源：[http](http://cheatsheets.aqeel-anwar.com/) ://cheatsheets.aqeel-anwar.com/ ）

# 概括

集成方法不是训练一个网络，而是使用多个弱学习器并聚合它们各自的输出以创建最终预测。下表显示了不同 Ensemble 方法的比较。

![img](https://miro.medium.com/max/1050/1*QvXYpo_7CV7rwqPbSCFy9A.png)

集成方法的比较（作者图片）